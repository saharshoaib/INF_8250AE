{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Notebook track - Extending PPO to a multi agent environment pistonballv6\n",
        "#Reinforcement Learning (RL) is a powerful approach for teaching agents to solve tasks through interaction with an environment. In this tutorial, we explore multi-agent RL and single-agent RL in the Pistonball environment using the PPO (Proximal Policy Optimization) algorithm. This tutorial highlights the versatility of RL in multi-agent and single-agent environments. By leveraging libraries like PettingZoo, SuperSuit, and Stable-Baselines3, we explore the nuances of policy learning in collaborative and isolated scenarios. The Pistonball environment provides an excellent playground for testing cooperative strategies and debugging individual agent behavior, making it ideal for RL experimentation and research.\n",
        "\n",
        "#The tutorial covers:\n",
        "\n",
        "#1)Single-Agent Training and Evaluation: Isolating a single piston and training it to act effectively in the environment.\n",
        "#2)Multi-Agent Training and Evaluation: Training all pistons collaboratively and recording their performance.\n",
        "\n",
        "#The Pistonball environment is a multi-agent environment from the PettingZoo library designed for collaborative tasks. Each agent (piston) moves vertically to bounce a ball toward the right. The goal is to keep the ball in the air and help it travel as far as possible while penalizing actions that waste energy. All pistons share a common goal, so performance depends on teamwork.Each agent sees a small window of the environment and the action space is treated as continuous.\n",
        "\n",
        "#Part one: This section focuses on isolating and training a single agent, piston_3, from the Pistonball environment. Using a custom SingleAgentWrapper, the multi-agent environment is adapted for single-agent training by isolating the observation, action, and reward spaces of the specified agent. The wrapper also records rendered frames for visualization.PPO is used to train the agent to maximize its individual contribution to the global task of moving the ball to the right. Training focuses solely on the selected agent's local interactions, simplifying the problem and reducing computational complexity.After training, the agent is evaluated in the same wrapped environment, and its performance is recorded as a video. This process provides insight into the agent's learned behavior, helping to debug and optimize its policy within the larger multi-agent system.\n",
        "#Part two: This section trains all agents in the Pistonball environment collaboratively using PPO. The environment is preprocessed with SuperSuit wrappers to standardize observation and action spaces, reduce complexity, and ensure compatibility with multi-agent RL.PPO trains a shared policy for all agents, optimizing their collective performance in moving the ball to the right while minimizing energy wastage. The algorithm balances exploration and exploitation, leveraging image-based inputs and temporal context provided by stacked frames. After training, the policy is evaluated, and the agents' collaborative behavior is recorded as a video. This allows for a visual assessment of the learned strategies and how well the agents work together to achieve the global goal.\n",
        "\n",
        "#Incomplete portion: extending the framework through custom policy to MADDPG (Multi-Agent Deep Deterministic Policy Gradient) from RLlib. Problem encountered with discrete space. A better alternative would have been perhaps DQN instead. More work to follow but i donot have enough time.\n",
        "\n"
      ],
      "metadata": {
        "id": "rRltcczRgqkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install pettingzoo\n",
        "!pip install stable_baselines3\n",
        "!pip install gym\n",
        "!pip install pymunk\n",
        "!pip install gymnasium\n",
        "!pip install supersuit\n",
        "!pip install pettingzoo stable-baselines3 supersuit\n",
        "!pip install pettingzoo supersuit stable-baselines3 gym\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install pettingzoo stable-baselines3 gymnasium shimmy supersuit\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "from stable_baselines3 import PPO\n",
        "from supersuit import pad_observations_v0, pad_action_space_v0\n",
        "from gymnasium import Env, spaces\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Step 1: Initialize PettingZoo AEC Environment with RGB Array Rendering\n",
        "env = pistonball_v6.env(render_mode=\"rgb_array\")  # Use rgb_array for frame rendering\n",
        "\n",
        "# Step 2: Apply Supersuit Wrappers\n",
        "env = pad_observations_v0(env)\n",
        "env = pad_action_space_v0(env)\n",
        "\n",
        "# Step 3: Single-Agent Wrapper for AEC Environment\n",
        "class SingleAgentWrapper(Env):\n",
        "    \"\"\"Wrapper to isolate a single agent and comply with Gym API.\"\"\"\n",
        "    def __init__(self, env, agent_id):\n",
        "        self.env = env\n",
        "        self.agent_id = agent_id\n",
        "        self.action_space = self.env.action_space(self.agent_id)\n",
        "        self.observation_space = self.env.observation_space(self.agent_id)\n",
        "        self.frames = []  # Store frames for video rendering\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.frames = []  # Clear previous frames\n",
        "        self.env.reset(seed=seed, options=options)\n",
        "        obs, _, _, _,_ = self.env.last()\n",
        "        self.frames.append(self.env.render())  # Save initial frame\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.env.step(action)\n",
        "        obs, reward, terminated, truncated, _ = self.env.last()\n",
        "        done = terminated or truncated\n",
        "        self.frames.append(self.env.render())  # Save current frame\n",
        "        return obs, reward, done, False, {}\n",
        "\n",
        "    def render_video(self, video_path=\"output.mp4\", fps=30):\n",
        "        \"\"\"Save the collected frames as a video file.\"\"\"\n",
        "        height, width, _ = self.frames[0].shape\n",
        "        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "        for frame in self.frames:\n",
        "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR\n",
        "            out.write(frame_bgr)\n",
        "        out.release()\n",
        "        print(f\"Video saved to {video_path}\")\n",
        "\n",
        "# Step 4: Wrap the Environment for Single Agent\n",
        "agent_id = \"piston_3\"  # Focused agent\n",
        "single_agent_env = SingleAgentWrapper(env, agent_id)\n",
        "\n",
        "# Ensure compatibility with Stable Baselines3\n",
        "vec_env = DummyVecEnv([lambda: single_agent_env])\n",
        "\n",
        "# Step 5: Train PPO Model\n",
        "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
        "print(\"Training PPO model...\")\n",
        "model.learn(total_timesteps=50000) #configurable param for better training\n",
        "\n",
        "# Step 6: Test the Trained Agent and Record Video\n",
        "print(\"Testing PPO model and recording video...\")\n",
        "obs = vec_env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, _ = vec_env.step(action)\n",
        "    if done:\n",
        "        obs = vec_env.reset()\n",
        "\n",
        "# Save the recorded video\n",
        "video_path = \"pistonball_output.mp4\"\n",
        "single_agent_env.render_video(video_path)\n",
        "\n",
        "# Step 7: Display Video in Colab\n",
        "def show_video(video_path):\n",
        "    \"\"\"Display video in Colab.\"\"\"\n",
        "    with open(video_path, \"rb\") as video_file:\n",
        "        video_data = video_file.read()\n",
        "    video_base64 = b64encode(video_data).decode()\n",
        "    video_html = f'''\n",
        "        <video width=\"600\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "    '''\n",
        "    return HTML(video_html)\n",
        "\n",
        "show_video(video_path)\n"
      ],
      "metadata": {
        "id": "FDck6azDS_WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install pettingzoo\n",
        "!pip install stable_baselines3\n",
        "!pip install gym\n",
        "!pip install pymunk\n",
        "!pip install gymnasium\n",
        "!pip install supersuit\n",
        "!pip install pettingzoo stable-baselines3 supersuit\n",
        "!pip install pettingzoo supersuit stable-baselines3 gym\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install pettingzoo stable-baselines3 gymnasium shimmy supersuit\n",
        "\n",
        "\n",
        "\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "from stable_baselines3.ppo import CnnPolicy\n",
        "from stable_baselines3 import PPO\n",
        "import supersuit as ss\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "import os\n",
        "\n",
        "#Step 1: Initialising training and evaluation\n",
        "# False: Training ; True: Evaluation\n",
        "is_evaluation = True\n",
        "\n",
        "#Step 2: Training the model\n",
        "def train():\n",
        "    # Set up the environment for training\n",
        "    env = pistonball_v6.parallel_env(n_pistons=20, time_penalty=-0.1, continuous=True,\n",
        "                                    random_drop=True, random_rotate=True, ball_mass=0.75, ball_friction=0.3,\n",
        "                                    ball_elasticity=1.5, max_cycles=125)\n",
        "\n",
        "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    env = ss.frame_stack_v1(env, 3)\n",
        "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class=\"stable_baselines3\")\n",
        "\n",
        "    # Define PPO model\n",
        "    model = PPO(CnnPolicy, env, verbose=3, gamma=0.95, n_steps=512, ent_coef=0.0905168,\n",
        "                learning_rate=0.00062211, vf_coef=0.042202, max_grad_norm=0.9, gae_lambda=0.99,\n",
        "                n_epochs=5, clip_range=0.3, batch_size=256)\n",
        "\n",
        "    # Training the model\n",
        "    print(\"\\nTraining is starting...\\n\")\n",
        "    model.learn(total_timesteps=200000)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save(\"/content/policy\")\n",
        "    print(\"\\nModel saved to /content/policy\\n\")\n",
        "\n",
        "#Step 3: Record Video\n",
        "def record_video(env, model, video_length=500, prefix=\"eval\"):\n",
        "    \"\"\"Records a video of the evaluation and saves it to /content/videos/.\"\"\"\n",
        "    video_folder = \"/content/videos/\"\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "    env = VecVideoRecorder(env, video_folder, record_video_trigger=lambda x: x == 0, video_length=video_length, name_prefix=prefix)\n",
        "\n",
        "    obs = env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, _, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    print(f\"\\nVideo recorded at {video_folder}\\n\")\n",
        "\n",
        "#Step 4: Set up the environment for evaluation\n",
        "def evaluate():\n",
        "\n",
        "    print(\"\\nEvaluation is starting...\\n\")\n",
        "    env = pistonball_v6.parallel_env(n_pistons=20, time_penalty=-0.1, continuous=True,\n",
        "                                     random_drop=True, random_rotate=True, ball_mass=0.75, ball_friction=0.3,\n",
        "                                     ball_elasticity=1.5, max_cycles=125, render_mode=\"rgb_array\")\n",
        "\n",
        "    # Apply preprocessing\n",
        "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    env = ss.frame_stack_v1(env, 3)\n",
        "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "    env = ss.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
        "\n",
        "\n",
        "    #env = pistonball_v6.env()\n",
        "    #env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    #env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    #env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = PPO.load(\"/content/policy\")\n",
        "\n",
        "    # Record a video of the evaluation\n",
        "    record_video(env, model, video_length=1000, prefix=\"pistonball_eval\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if is_evaluation:\n",
        "        evaluate()\n",
        "    else:\n",
        "        train()\n"
      ],
      "metadata": {
        "id": "6dPHWbkWf9Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#UNFINISHED\n",
        "\n",
        "!pip install pettingzoo supersuit ray[rllib]\n",
        "!pip install ray[rllib]==2.0.0\n",
        "!pip install open-cv-python-headless\n",
        "\n",
        "#!pip uninstall pydantic\n",
        "!pip install pydantic==1.10.12\n",
        "from torch import nn\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
        "from supersuit import color_reduction_v0, resize_v1, frame_stack_v1, normalize_obs_v0\n",
        "from ray.rllib.algorithms.maddpg import MADDPGConfig\n",
        "from ray.tune.registry import register_env\n",
        "import ray\n",
        "from ray.rllib.algorithms.dqn import DQN\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from supersuit import pad_observations_v0, pad_action_space_v0\n",
        "\n",
        "\n",
        "import supersuit as ss\n",
        "\n",
        "\n",
        "class CNNModelV2(TorchModelV2, nn.Module):\n",
        "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
        "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
        "        nn.Module.__init__(self)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            (nn.Linear(3136, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy_fn = nn.Linear(512, num_outputs)\n",
        "        self.value_fn = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
        "        self._value_out = self.value_fn(model_out)\n",
        "        return self.policy_fn(model_out), state\n",
        "\n",
        "    def value_function(self):\n",
        "        return self._value_out.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_environment():\n",
        "    env = pistonball_v6.env(\n",
        "        n_pistons=20,\n",
        "        time_penalty=-0.1,\n",
        "        continuous=True,\n",
        "        random_drop=True,\n",
        "        random_rotate=True,\n",
        "        ball_mass=0.75,\n",
        "        ball_friction=0.3,\n",
        "        ball_elasticity=1.5,\n",
        "        max_cycles=125,\n",
        "        render_mode=\"rgb_array\",\n",
        "    )\n",
        "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    env = ss.dtype_v0(env, \"float32\")\n",
        "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
        "    env = ss.frame_stack_v1(env, 3)\n",
        "    #env = pad_observations_v0(env)\n",
        "    env = pad_action_space_v0(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "# Create the environment and apply Supersuit wrappers\n",
        "#def create_environment():\n",
        "    # Load Pistonball environment\n",
        "    #env = pistonball_v6.parallel_env()\n",
        "    # Apply Supersuit wrappers to modify the environment (preprocessing)\n",
        "    #env = color_reduction_v0(env, mode=\"B\")  # Reduce observation complexity (e.g., grayscale)\n",
        "    #env = resize_v1(env, x_size=84, y_size=84)  # Resize observation space\n",
        "    #env = frame_stack_v1(env, stack_size=4)  # Stack frames for temporal context\n",
        "    #return env\n",
        "\n",
        "# Wrapper to register the environment with RLlib\n",
        "def env_creator(config):\n",
        "    return create_environment()\n",
        "\n",
        "# Training configuration and loop\n",
        "def train_marl_agents():\n",
        "    # Initialize Ray\n",
        "    ray.init(ignore_reinit_error=True)\n",
        "\n",
        "    # Register the environment with RLlib\n",
        "    env_name = \"Pistonball-v6\"\n",
        "    register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "    ModelCatalog.register_custom_model(\"custom_cnn\", CNNModelV2)\n",
        "\n",
        "\n",
        "    #register_env(env_name, lambda config: PettingZooEnv(pistonball_v6.env()))\n",
        "    #register_env(env_name, env_creator)\n",
        "    temp_env = create_environment()\n",
        "    temp_env.reset()\n",
        "    agent_ids = temp_env.possible_agents\n",
        "\n",
        "\n",
        "\n",
        "    obs_spaces = {agent_id: temp_env.observation_space(agent_id) for agent_id in agent_ids}\n",
        "    act_spaces = {agent_id: temp_env.action_space(agent_id) for agent_id in agent_ids}\n",
        "\n",
        "    string_to_int_mapping = {agent_id: idx for idx, agent_id in enumerate(temp_env.possible_agents)}\n",
        "\n",
        "    # Define policies with string-based IDs\n",
        "    policies = {\n",
        "        f\"policy_{idx}\": (\n",
        "            None,\n",
        "            temp_env.observation_space(agent_id),\n",
        "            temp_env.action_space(agent_id),\n",
        "            {\n",
        "                \"agent_id\": idx,  # Integer-based agent ID for MADDPG\n",
        "                \"model\": {\n",
        "                    \"custom_model\": \"custom_cnn\",  # Custom CNN model\n",
        "                },\n",
        "            },\n",
        "        )\n",
        "        for agent_id, idx in string_to_int_mapping.items()\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Define policies using the policy creation function\n",
        "\n",
        "    config = (\n",
        "        MADDPGConfig()\n",
        "        .environment(env=\"pistonball\")  # Replace with your registered environment name\n",
        "        .framework(\"torch\")\n",
        "        .rollouts(num_rollout_workers=2)\n",
        "        .training(\n",
        "            model={\n",
        "                \"custom_model\": \"custom_cnn\",  # Reference the registered custom CNN\n",
        "            },\n",
        "            multiagent={\n",
        "                policies==policies,\n",
        "                policy_mapping_fn=lambda agent_id, *args, **kwargs: f\"policy_{string_to_int_mapping[agent_id]}\",\n",
        "                # Map each agent to its own policy\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "    algo = config.build()\n",
        "\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Configure the MADDPG algorithm\n",
        "\n",
        "    #config = (\n",
        "\n",
        "        #MADDPGConfig()\n",
        "        #.environment(env=env_name)\n",
        "        #.rollouts(rollout_fragment_length=200)\n",
        "        #.multi_agent(\n",
        "            #policies=policies,\n",
        "            #policy_mapping_fn=lambda agent_id, *args, **kwargs: f\"policy_{string_to_int_mapping[agent_id]}\",\n",
        "            # Map each agent to its own policy\n",
        "        #)\n",
        "        #.framework(\"torch\")\n",
        "        #.resources(num_gpus=1)  # Set to 0 if GPU is not available\n",
        "    #)\n",
        "\n",
        "    # Build the algorithm\n",
        "    #algo = config.build()\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(1000):  # Adjust iterations as needed\n",
        "        result = algo.train()\n",
        "        print(f\"Iteration {i}: reward = {result['episode_reward_mean']}\")\n",
        "\n",
        "        # Save the model periodically\n",
        "        if i % 100 == 0:\n",
        "            algo.save(f\"checkpoint_{i}\")\n",
        "\n",
        "    # Save the final model\n",
        "    algo.save(\"final_marl_model\")\n",
        "    print(\"Training complete. Model saved.\")\n",
        "\n",
        "    ray.shutdown()\n",
        "    return algo\n",
        "\n",
        "# Evaluation loop with video rendering\n",
        "def evaluate_marl_agents(algo, num_episodes=5, video_dir=\"videos\"):\n",
        "    import os\n",
        "    import cv2\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    env = create_environment()\n",
        "\n",
        "    # Ensure video directory exists\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        env.reset()\n",
        "        frames = []\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            observation, reward, done, info = env.last()\n",
        "            action = algo.compute_action(observation) if not done else None\n",
        "            env.step(action)\n",
        "            frames.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        # Save the episode as a video\n",
        "        video_path = os.path.join(video_dir, f\"episode_{episode + 1}.mp4\")\n",
        "        save_video(frames, video_path)\n",
        "        print(f\"Saved episode {episode + 1} video at {video_path}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "# Function to save video from frames\n",
        "def save_video(frames, video_path, fps=30):\n",
        "    height, width, _ = frames[0].shape\n",
        "    video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
        "    for frame in frames:\n",
        "        video.write(frame)\n",
        "    video.release()\n",
        "    print(f\"Video saved to {video_path}\")\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the agents\n",
        "    algo = train_marl_agents()\n",
        "\n",
        "    # Evaluate the trained model and render videos\n",
        "    evaluate_marl_agents(algo)"
      ],
      "metadata": {
        "id": "dsP-HhZKmIEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}